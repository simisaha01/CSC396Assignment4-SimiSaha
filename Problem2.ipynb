{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70089b5b",
   "metadata": {},
   "source": [
    "# Using Pre-trained Word Embeddings\n",
    "\n",
    "In this notebook we will show some operations on pre-trained word embeddings to gain an intuition about them.\n",
    "\n",
    "We will be using the pre-trained GloVe embeddings that can be found in the [official website](https://nlp.stanford.edu/projects/glove/). In particular, we will use the file `glove.6B.300d.txt` contained in this [zip file](https://nlp.stanford.edu/data/glove.6B.zip).\n",
    "\n",
    "We will first load the GloVe embeddings using [Gensim](https://radimrehurek.com/gensim/). Specifically, we will use [`KeyedVectors`](https://radimrehurek.com/gensim/models/keyedvectors.html)'s [`load_word2vec_format()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.load_word2vec_format) classmethod, which supports the original word2vec file format.\n",
    "However, there is a difference in the file formats used by GloVe and word2vec, which is a header used by word2vec to indicate the number of embeddings and dimensions stored in the file. The file that stores the GloVe embeddings doesn't have this header, so we will have to address that when loading the embeddings.\n",
    "\n",
    "Loading the embeddings may take a little bit, so hang in there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31adf8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.12.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/61.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wrapt, smart-open, gensim\n",
      "Successfully installed gensim-4.3.3 smart-open-7.0.5 wrapt-1.17.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "fname = \"glove.6B.300d-vocabulary.txt\"\n",
    "glove = KeyedVectors.load_word2vec_format(fname, no_header=True)\n",
    "glove.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3691c66",
   "metadata": {},
   "source": [
    "## Word similarity\n",
    "\n",
    "One attribute of word embeddings that makes them useful is the ability to compare them using cosine similarity to find how similar they are. [`KeyedVectors`](https://radimrehurek.com/gensim/models/keyedvectors.html) objects provide a method called [`most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) that we can use to find the closest words to a particular word of interest. By default, [`most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) returns the 10 most similar words, but this can be changed using the `topn` parameter.\n",
    "\n",
    "Below we test this function using a few different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "592b5236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('5,770', nan),\n",
       " ('woundwort', nan),\n",
       " ('40.64', nan),\n",
       " ('hookey', nan),\n",
       " ('tigerman', nan),\n",
       " ('jagua', nan),\n",
       " ('kissane', nan),\n",
       " ('bawean', nan),\n",
       " ('5,430', nan),\n",
       " ('termly', nan)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# common noun\n",
    "glove.most_similar(\"cactus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9c5ded2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5,770', nan),\n",
       " ('woundwort', nan),\n",
       " ('40.64', nan),\n",
       " ('hookey', nan),\n",
       " ('tigerman', nan),\n",
       " ('jagua', nan),\n",
       " ('kissane', nan),\n",
       " ('bawean', nan),\n",
       " ('5,430', nan),\n",
       " ('termly', nan)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# common noun\n",
    "glove.most_similar(\"cake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca890ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5,770', nan),\n",
       " ('woundwort', nan),\n",
       " ('40.64', nan),\n",
       " ('hookey', nan),\n",
       " ('tigerman', nan),\n",
       " ('jagua', nan),\n",
       " ('kissane', nan),\n",
       " ('bawean', nan),\n",
       " ('5,430', nan),\n",
       " ('termly', nan)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adjective\n",
    "glove.most_similar(\"angry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a297340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5,770', nan),\n",
       " ('woundwort', nan),\n",
       " ('40.64', nan),\n",
       " ('hookey', nan),\n",
       " ('tigerman', nan),\n",
       " ('jagua', nan),\n",
       " ('kissane', nan),\n",
       " ('bawean', nan),\n",
       " ('5,430', nan),\n",
       " ('termly', nan)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adverb\n",
    "glove.most_similar(\"quickly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33dfe403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5,770', nan),\n",
       " ('woundwort', nan),\n",
       " ('40.64', nan),\n",
       " ('hookey', nan),\n",
       " ('tigerman', nan),\n",
       " ('jagua', nan),\n",
       " ('kissane', nan),\n",
       " ('bawean', nan),\n",
       " ('5,430', nan),\n",
       " ('termly', nan)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preposition\n",
    "glove.most_similar(\"between\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1919d8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5,770', nan),\n",
       " ('woundwort', nan),\n",
       " ('40.64', nan),\n",
       " ('hookey', nan),\n",
       " ('tigerman', nan),\n",
       " ('jagua', nan),\n",
       " ('kissane', nan),\n",
       " ('bawean', nan),\n",
       " ('5,430', nan),\n",
       " ('termly', nan)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determiner\n",
    "glove.most_similar(\"the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef3da4",
   "metadata": {},
   "source": [
    "## Word analogies\n",
    "\n",
    "Another characteristic of word embeddings is their ability to solve analogy problems.\n",
    "The same [`most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) method can be used for this task, by passing two lists of words:\n",
    "a `positive` list with the words that should be added and a `negative` list with the words that should be subtracted. Using these arguments, the famous example $\\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}$ can be executed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4badb9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5,770', nan),\n",
       " ('woundwort', nan),\n",
       " ('40.64', nan),\n",
       " ('hookey', nan),\n",
       " ('tigerman', nan),\n",
       " ('jagua', nan),\n",
       " ('kissane', nan),\n",
       " ('bawean', nan),\n",
       " ('5,430', nan),\n",
       " ('termly', nan)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# king - man + woman\n",
    "glove.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5619e",
   "metadata": {},
   "source": [
    "Here are a few other interesting analogies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b357d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5,770', nan),\n",
       " ('woundwort', nan),\n",
       " ('40.64', nan),\n",
       " ('hookey', nan),\n",
       " ('tigerman', nan),\n",
       " ('jagua', nan),\n",
       " ('kissane', nan),\n",
       " ('bawean', nan),\n",
       " ('5,430', nan),\n",
       " ('termly', nan)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# car - drive + fly\n",
    "glove.most_similar(positive=[\"car\", \"fly\"], negative=[\"drive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceed96a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5,770', nan),\n",
       " ('woundwort', nan),\n",
       " ('40.64', nan),\n",
       " ('hookey', nan),\n",
       " ('tigerman', nan),\n",
       " ('jagua', nan),\n",
       " ('kissane', nan),\n",
       " ('bawean', nan),\n",
       " ('5,430', nan),\n",
       " ('termly', nan)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# berlin - germany + australia\n",
    "glove.most_similar(positive=[\"berlin\", \"australia\"], negative=[\"germany\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "334606b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5,770', nan),\n",
       " ('woundwort', nan),\n",
       " ('40.64', nan),\n",
       " ('hookey', nan),\n",
       " ('tigerman', nan),\n",
       " ('jagua', nan),\n",
       " ('kissane', nan),\n",
       " ('bawean', nan),\n",
       " ('5,430', nan),\n",
       " ('termly', nan)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# england - london + baghdad\n",
    "glove.most_similar(positive=[\"england\", \"baghdad\"], negative=[\"london\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5d8a5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5,770', nan),\n",
       " ('woundwort', nan),\n",
       " ('40.64', nan),\n",
       " ('hookey', nan),\n",
       " ('tigerman', nan),\n",
       " ('jagua', nan),\n",
       " ('kissane', nan),\n",
       " ('bawean', nan),\n",
       " ('5,430', nan),\n",
       " ('termly', nan)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# japan - yen + peso\n",
    "glove.most_similar(positive=[\"japan\", \"peso\"], negative=[\"yen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78e0663d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5,770', nan),\n",
       " ('woundwort', nan),\n",
       " ('40.64', nan),\n",
       " ('hookey', nan),\n",
       " ('tigerman', nan),\n",
       " ('jagua', nan),\n",
       " ('kissane', nan),\n",
       " ('bawean', nan),\n",
       " ('5,430', nan),\n",
       " ('termly', nan)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best - good + tall\n",
    "glove.most_similar(positive=[\"best\", \"tall\"], negative=[\"good\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82403941",
   "metadata": {},
   "source": [
    "## Looking under the hood\n",
    "\n",
    "Now that we are more familiar with the [`most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) method, it is time to implement its functionality ourselves.\n",
    "But first, we need to take a look at the different parts of the [`KeyedVectors`](https://radimrehurek.com/gensim/models/keyedvectors.html) object that we will need.\n",
    "Obviously, we will need the vectors themselves. They are stored in the `vectors` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "153802bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc372d2e",
   "metadata": {},
   "source": [
    "As we can see above, `vectors` is a 2-dimensional matrix with 400,000 rows and 300 columns.\n",
    "Each row corresponds to a 300-dimensional word embedding. These embeddings are not normalized, but normalized embeddings can be obtained using the [`get_normed_vectors()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_normed_vectors) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d61d4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_vectors = glove.get_normed_vectors()\n",
    "normed_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24407ad7",
   "metadata": {},
   "source": [
    "Now we need to map the words in the vocabulary to rows in the `vectors` matrix, and vice versa.\n",
    "The [`KeyedVectors`](https://radimrehurek.com/gensim/models/keyedvectors.html) object has the attributes `index_to_key` and `key_to_index` which are a list of words and a dictionary of words to indices, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9280e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4695b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef07a09",
   "metadata": {},
   "source": [
    "## Word similarity from scratch\n",
    "\n",
    "Now we have everything we need to implement a `most_similar_words()` function that takes a word, the vector matrix, the `index_to_key` list, and the `key_to_index` dictionary. This function will return the 10 most similar words to the provided word, along with their similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b3aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97cc95993c2544739939e42c22f7b2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a283c18c77f14720a37a07cf83e6545f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4b953ca0c6445e9c6d174ac61dfdd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c22c8bb9b24015b942369b462f00c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b69350c5ab483198c7e89a0c583bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def compute_word_embeddings(vocab_file, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Compute word embeddings by averaging token embeddings.\n",
    "    \"\"\"\n",
    "    word_embeddings = {}\n",
    "    with open(vocab_file, 'r') as f:\n",
    "        for line in f:\n",
    "            word = line.strip()\n",
    "            # Tokenize word\n",
    "            tokens = tokenizer(word, return_tensors='pt', add_special_tokens=False)\n",
    "            token_ids = tokens['input_ids'][0]\n",
    "            \n",
    "            # Get embeddings for tokens\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**tokens)\n",
    "                embeddings = outputs.last_hidden_state[0]  # Shape: (num_tokens, hidden_size)\n",
    "            \n",
    "            # Average token embeddings to form word embedding\n",
    "            word_embedding = embeddings.mean(dim=0).numpy()\n",
    "            word_embeddings[word] = word_embedding\n",
    "    \n",
    "    return word_embeddings\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def most_similar(word, word_embeddings, top_n=5):\n",
    "    if word not in word_embeddings:\n",
    "        raise ValueError(f\"Word '{word}' not in vocabulary.\")\n",
    "    \n",
    "    target_embedding = word_embeddings[word].reshape(1, -1)\n",
    "    similarities = {}\n",
    "    \n",
    "    for other_word, embedding in word_embeddings.items():\n",
    "        similarity = cosine_similarity(target_embedding, embedding.reshape(1, -1))[0][0]\n",
    "        similarities[other_word] = similarity\n",
    "\n",
    "    return sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "\n",
    "# Compute word embeddings\n",
    "vocab_file = \"glove.6B.300d-vocabulary.txt\"\n",
    "word_embeddings = compute_word_embeddings(vocab_file, model, tokenizer)\n",
    "\n",
    "# Run examples\n",
    "examples = ['king', 'queen', 'man', 'woman', 'apple', 'banana']\n",
    "results = {}\n",
    "for example in examples:\n",
    "    results[example] = most_similar_words(example, word_embeddings)\n",
    "\n",
    "# Display results\n",
    "for word, similar in results.items():\n",
    "    print(f\"Most similar to '{word}':\")\n",
    "    for sim_word, score in similar:\n",
    "        print(f\"  {sim_word}: {score:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad469a",
   "metadata": {},
   "source": [
    "Now let's try the same example that we used above: the most similar words to \"cactus\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea29a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = glove.get_normed_vectors()\n",
    "index_to_key = glove.index_to_key\n",
    "key_to_index = glove.key_to_index\n",
    "most_similar_words(\"cactus\", vectors, index_to_key, key_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bfd0c3",
   "metadata": {},
   "source": [
    "## Analogies from scratch\n",
    "\n",
    "The `most_similar_words()` function behaves as expected. Now let's implement a function to perform the analogy task. We will give it the very creative name `analogy`. This function will get two lists of words (one for positive words and one for negative words), just like the [`most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) method we discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def analogy(positive, negative, vectors, index_to_key, key_to_index, topn=10):\n",
    "    # find ids for positive and negative words\n",
    "    pos_ids = [key_to_index[w] for w in positive]\n",
    "    neg_ids = [key_to_index[w] for w in negative]\n",
    "    given_word_ids = pos_ids + neg_ids\n",
    "    # get embeddings for positive and negative words\n",
    "    pos_emb = vectors[pos_ids].sum(axis=0)\n",
    "    neg_emb = vectors[neg_ids].sum(axis=0)\n",
    "    # get embedding for analogy\n",
    "    emb = pos_emb - neg_emb\n",
    "    # normalize embedding\n",
    "    emb = emb / norm(emb)\n",
    "    # calculate similarities to all words in out vocabulary\n",
    "    similarities = vectors @ emb\n",
    "    # get word_ids in ascending order with respect to similarity score\n",
    "    ids_ascending = similarities.argsort()\n",
    "    # reverse word_ids\n",
    "    ids_descending = ids_ascending[::-1]\n",
    "    # get boolean array with element corresponding to any of given_word_ids set to false\n",
    "    given_words_mask = np.isin(ids_descending, given_word_ids, invert=True)\n",
    "    # obtain new array of indices that doesn't contain any of the given_word_ids\n",
    "    ids_descending = ids_descending[given_words_mask]\n",
    "    # get topn word_ids\n",
    "    top_ids = ids_descending[:topn]\n",
    "    # retrieve topn words with their corresponding similarity score\n",
    "    top_words = [(index_to_key[i], similarities[i]) for i in top_ids]\n",
    "    # return results\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a9cff3",
   "metadata": {},
   "source": [
    "Let's try this function with the $\\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}$ example we discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c103f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = [\"king\", \"woman\"]\n",
    "negative = [\"man\"]\n",
    "vectors = glove.get_normed_vectors()\n",
    "index_to_key = glove.index_to_key\n",
    "key_to_index = glove.key_to_index\n",
    "analogy(positive, negative, vectors, index_to_key, key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d81ff25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
